from playwright.sync_api import sync_playwright
from bs4 import BeautifulSoup
from urllib.parse import urljoin, urlparse

def extract_article_links(index_url):
    print(f"ðŸ”— Accesez cu Playwright: {index_url}")

    # ðŸ”¹ ListÄƒ flexibilÄƒ de cuvinte care apar frecvent Ã®n linkurile articolelor
    link_filters = ["/solutions/", "/blog/", "/news/", "/article", "/post", "/insights", "/update"]

    try:
        with sync_playwright() as p:
            browser = p.chromium.launch(headless=True)
            page = browser.new_page()
            page.goto(index_url, timeout=60000)

            # AÈ™teptÄƒm pentru Ã®ncÄƒrcare completÄƒ
            page.wait_for_timeout(7000)
            page.screenshot(path="debug.png")  # opÈ›ional pentru verificare

            html = page.content()
            browser.close()

    except Exception as e:
        print(f"âš ï¸ Eroare Playwright: {e}")
        return []

    soup = BeautifulSoup(html, "html.parser")
    links = []

    parsed_base = urlparse(index_url)
    base_domain = parsed_base.netloc

    for a in soup.find_all("a", href=True):
        href = a["href"]
        full_url = urljoin(index_url, href)
        parsed = urlparse(full_url)

        # ðŸ”¸ Reguli de filtrare inteligente:
        if (
            parsed.netloc == base_domain and
            any(f in parsed.path for f in link_filters) and
            not href.startswith("#") and
            not parsed.path.endswith("/") and
            len(parsed.path.strip("/").split("/")) >= 2  # minim 2 niveluri Ã®n path
        ):
            links.append(full_url)

    unique_links = list(set(links))
    print(f"âœ… GÄƒsite {len(unique_links)} linkuri care trec filtrul flexibil.")
    return unique_links
